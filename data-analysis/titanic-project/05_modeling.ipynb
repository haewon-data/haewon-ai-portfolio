{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf503a60",
   "metadata": {},
   "source": [
    "# ðŸ¤– Modeling\n",
    "\n",
    "This notebook focuses on building and evaluating predictive models  \n",
    "for passenger survival using the engineered Titanic dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Purpose\n",
    "\n",
    "To establish a baseline model and compare multiple classification algorithms  \n",
    "using engineered features created in the previous notebook.\n",
    "\n",
    "## ðŸ“¦ Dataset\n",
    "\n",
    "Same dataset used in previous notebooks:  \n",
    "[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)  \n",
    "via public repository: [Data Science Dojo GitHub](https://github.com/datasciencedojo/datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d24a07",
   "metadata": {},
   "source": [
    "ðŸ“¦ 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674f968",
   "metadata": {},
   "source": [
    "We begin by loading the feature-engineered dataset saved from the previous notebook.  \n",
    "This dataset contains all processed and engineered features, ready for modeling.\n",
    "\n",
    "Some models cannot handle string-type features.  \n",
    "We prepare two versions of the feature set:\n",
    "\n",
    "- `X_full`: includes all engineered features  \n",
    "- `X_safe`: excludes object-type columns like `Cabin` and `Title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1e666373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv(\"feature_engineered_titanic.csv\")\n",
    "\n",
    "# Target variable\n",
    "y = df['Survived']\n",
    "\n",
    "# Feature sets\n",
    "X_full = df.drop(columns=['Survived', 'Name', 'Ticket', 'PassengerId'])  # for tree-based models\n",
    "X_safe = X_full.drop(columns=['Cabin', 'Title'])                         # for models that require numeric inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd4676",
   "metadata": {},
   "source": [
    "ðŸ“ˆ 2. Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa639f",
   "metadata": {},
   "source": [
    "Logistic regression is a simple and interpretable classification algorithm,  \n",
    "making it a great starting point for binary classification tasks like survival prediction.\n",
    "\n",
    "However, it requires all input features to be numeric.  \n",
    "We use the `X_safe` feature set prepared earlier, which excludes non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ede1ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.8045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the safe feature set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_safe, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(f\"Baseline Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493d934",
   "metadata": {},
   "source": [
    "ðŸ“š 3. Compare Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99cd4ff",
   "metadata": {},
   "source": [
    "In this section, we train and evaluate several classification algorithms  \n",
    "to compare their predictive performance on the Titanic dataset.\n",
    "\n",
    "In the previous section, we established a **baseline** using Logistic Regression,  \n",
    "which is simple and interpretable but requires stricter preprocessing.\n",
    "\n",
    "Now, we move on to additional models â€” many of which can handle categorical variables more flexibly â€”  \n",
    "to see if they can outperform our baseline.\n",
    "\n",
    "We will evaluate the following models:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- K-Nearest Neighbors\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3792fbe",
   "metadata": {},
   "source": [
    "3-1. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f293a08",
   "metadata": {},
   "source": [
    "Decision trees split the dataset based on feature values  \n",
    "to make predictions through a series of learned rules.\n",
    "\n",
    "They are intuitive and can handle both numerical and categorical variables (if encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "85d78ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a decision tree using the same train/test split\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_tree):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a66621",
   "metadata": {},
   "source": [
    "3-2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e8188",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble method that combines multiple decision trees  \n",
    "to improve accuracy and reduce overfitting.\n",
    "\n",
    "It works well with both numerical and encoded categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c105cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a random forest model using the same train/test split\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29f35e",
   "metadata": {},
   "source": [
    "3-3. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fdfad",
   "metadata": {},
   "source": [
    "Gradient Boosting builds trees sequentially,  \n",
    "with each tree correcting the errors of the previous one.\n",
    "\n",
    "It often achieves strong performance on structured datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7c069e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a gradient boosting model using the same train/test split\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b00873",
   "metadata": {},
   "source": [
    "3-4. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22030580",
   "metadata": {},
   "source": [
    "KNN classifies data based on the majority label among its nearest neighbors.  \n",
    "It is simple but sensitive to feature scaling and works best with numeric inputs.\n",
    "\n",
    "We will use the `X_safe` feature set to ensure all features are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f27064fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.7318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a KNN model using numeric-safe features\n",
    "X_train_knn, X_test_knn, _, _ = train_test_split(X_safe, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_knn, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_knn = knn.predict(X_test_knn)\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e487187",
   "metadata": {},
   "source": [
    "3-5. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9292e",
   "metadata": {},
   "source": [
    "XGBoost is a powerful and efficient gradient boosting implementation  \n",
    "that often delivers high performance on structured data.\n",
    "\n",
    "It requires all input features to be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e7d3bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train XGBoost using numeric-safe features\n",
    "X_train_xgb, X_test_xgb, _, _ = train_test_split(X_safe, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb.predict(X_test_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f643cf0",
   "metadata": {},
   "source": [
    "ðŸ“Š 4. Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131e32a",
   "metadata": {},
   "source": [
    "We summarize the accuracy scores of all models tested above.  \n",
    "This allows us to compare their relative performance and identify which performed best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c350e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.821229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.804469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.798883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.787709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.770950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <td>0.731844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy\n",
       "Gradient Boosting    0.821229\n",
       "Logistic Regression  0.804469\n",
       "XGBoost              0.798883\n",
       "Random Forest        0.787709\n",
       "Decision Tree        0.770950\n",
       "K-Nearest Neighbors  0.731844"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect all model scores\n",
    "model_scores = {\n",
    "    \"Logistic Regression\": accuracy_score(y_test, y_pred),\n",
    "    \"Decision Tree\": accuracy_score(y_test, y_pred_tree),\n",
    "    \"Random Forest\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"Gradient Boosting\": accuracy_score(y_test, y_pred_gb),\n",
    "    \"K-Nearest Neighbors\": accuracy_score(y_test, y_pred_knn),\n",
    "    \"XGBoost\": accuracy_score(y_test, y_pred_xgb),\n",
    "}\n",
    "\n",
    "# Create dataframe and sort by accuracy\n",
    "results_df = pd.DataFrame.from_dict(model_scores, orient='index', columns=['Accuracy'])\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4107a65",
   "metadata": {},
   "source": [
    "## ðŸ§  Summary\n",
    "\n",
    "In this notebook, I built and compared several machine learning models  \n",
    "to predict survival on the Titanic dataset.\n",
    "\n",
    "Here's what I worked on:\n",
    "\n",
    "- Loaded the processed dataset with all engineered features\n",
    "- Prepared two versions of the input data:  \n",
    "  `X_full` for tree-based models and `X_safe` for models that require numeric input\n",
    "- Started with a simple logistic regression as a baseline\n",
    "- Then tested five other models:\n",
    "  - Decision Tree\n",
    "  - Random Forest\n",
    "  - Gradient Boosting\n",
    "  - K-Nearest Neighbors\n",
    "  - XGBoost\n",
    "- Measured and compared their accuracy on the test set\n",
    "\n",
    "Among them, **Gradient Boosting** performed the best with an accuracy of 0.82,  \n",
    "slightly ahead of Logistic Regression (0.80) and XGBoost (0.80).\n",
    "\n",
    "This gives a good sense of how different models behave on this dataset,  \n",
    "and sets the stage for deeper evaluation or final model selection in the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d8035b",
   "metadata": {},
   "source": [
    "# üõ≥Ô∏è Final Submission\n",
    "\n",
    "This notebook wraps up the Titanic survival prediction project.  \n",
    "After evaluating all candidate models, we select the best-performing one, generate the final `.csv` file for submission,<br> and summarize key takeaways from the project.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "To finalize the project by:\n",
    "- Selecting the most suitable model based on evaluation metrics  \n",
    "- Generating the submission file in the required format  \n",
    "- Reflecting briefly on the overall process\n",
    "\n",
    "## üì¶ Dataset\n",
    "\n",
    "Same processed dataset used in earlier notebooks:  \n",
    "[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)  \n",
    "via public repository: [Data Science Dojo GitHub](https://github.com/datasciencedojo/datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35c339",
   "metadata": {},
   "source": [
    "üì¶ 1. Load the Dataset & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36293f5",
   "metadata": {},
   "source": [
    "We load the same processed dataset and reuse the train/test split  \n",
    "to ensure consistency with the previous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83f49d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load processed dataset\n",
    "df = pd.read_csv(\"feature_engineered_titanic.csv\")\n",
    "\n",
    "# Target and features\n",
    "y = df['Survived']\n",
    "X_full = df.drop(columns=['Survived', 'Name', 'Ticket', 'PassengerId'])\n",
    "X_safe = X_full.drop(columns=['Cabin', 'Title'])\n",
    "\n",
    "# Shared train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_safe, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ddc16",
   "metadata": {},
   "source": [
    "ü§ñ 2. Train the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebb7ad",
   "metadata": {},
   "source": [
    "Based on the evaluation results,  \n",
    "we select `GradientBoostingClassifier` as the final model due to its consistently strong performance across all metrics, including F1 Score and AUC.\n",
    "\n",
    "We'll now retrain it on the training set and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99191064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train the final model\n",
    "final_model = GradientBoostingClassifier(random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "final_preds = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9dbfa",
   "metadata": {},
   "source": [
    "üìÑ 3. Generate final submission file (for Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58e1b7",
   "metadata": {},
   "source": [
    "To prepare for submission, we retrain the final model using the full training set  \n",
    "and generate predictions on the processed test set.\n",
    "\n",
    "We apply the same feature engineering pipeline to the test set using a custom preprocessing function.\n",
    "\n",
    "Before prediction, we ensure the test data:\n",
    "- Has all the features used in training (aligned by column order)\n",
    "- Contains only numeric values (with categorical features properly encoded)\n",
    "\n",
    "The output follows Kaggle's required format:\n",
    "- `PassengerId`: from the original `test.csv`\n",
    "- `Survived`: predicted survival (0 or 1)\n",
    "\n",
    "The final file will be saved as `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e7e18098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_for_submission(df):\n",
    "    # Fill missing values\n",
    "    df[\"Age\"] = df.groupby([\"Pclass\", \"Sex\"])[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "    # Title extraction & simplification\n",
    "    df[\"Title\"] = df[\"Name\"].str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    df[\"Title\"] = df[\"Title\"].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don',\n",
    "                                       'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df[\"Title\"] = df[\"Title\"].replace(['Mlle', 'Ms'], 'Miss')\n",
    "    df[\"Title\"] = df[\"Title\"].replace('Mme', 'Mrs')\n",
    "\n",
    "    # Family features\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "\n",
    "    # Additional engineered features (same as training)\n",
    "    df[\"LowFare\"] = (df[\"Fare\"] <= 7.91).astype(int)\n",
    "    df[\"AgeBin\"] = pd.cut(df[\"Age\"], bins=[0, 16, 32, 48, 64, 80], labels=False)\n",
    "    df[\"ModerateFamily\"] = df[\"FamilySize\"].apply(lambda x: 1 if 2 <= x <= 4 else 0)\n",
    "    df[\"IsCherbourg\"] = (df[\"Embarked\"] == \"C\").astype(int)\n",
    "    df[\"FemaleFirstSecondClass\"] = ((df[\"Sex\"] == \"female\") & (df[\"Pclass\"] != 3)).astype(int)\n",
    "    df[\"IsChildOrElderly\"] = ((df[\"Age\"] <= 10) | (df[\"Age\"] >= 60)).astype(int)\n",
    "    df[\"LowFare_3rdClass\"] = ((df[\"Pclass\"] == 3) & (df[\"Fare\"] < 7.91)).astype(int)\n",
    "\n",
    "    # One-hot encoding: Pclass\n",
    "    df = pd.get_dummies(df, columns=[\"Pclass\"], prefix=\"Pclass\")\n",
    "    # One-hot encoding: Embarked\n",
    "    df = pd.get_dummies(df, columns=[\"Embarked\"], prefix=\"Embarked\")\n",
    "    # One-hot encoding: Cabin first letter\n",
    "    df[\"Cabin\"] = df[\"Cabin\"].fillna(\"U\")\n",
    "    df[\"Cabin\"] = df[\"Cabin\"].str[0]\n",
    "    df = pd.get_dummies(df, columns=[\"Cabin\"], prefix=\"Cabin\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df.drop(columns=[\"Name\", \"Ticket\", \"PassengerId\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73b3662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "X_kaggle = preprocess_test_for_submission(df_test.copy())\n",
    "X_kaggle.to_csv(\"feature_engineered_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d3d54412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data\n",
    "df_train = pd.read_csv(\"feature_engineered_titanic.csv\")\n",
    "X_all = df_train.drop(columns=['Survived', 'Name', 'Ticket', 'PassengerId', 'Cabin', 'Title'])\n",
    "y_all = df_train['Survived']\n",
    "\n",
    "# Load Kaggle test data\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "X_kaggle = pd.read_csv(\"feature_engineered_test.csv\") \n",
    "\n",
    "# Fix object-type columns manually\n",
    "X_kaggle[\"Sex\"] = X_kaggle[\"Sex\"].map({\"male\": 0, \"female\": 1}).astype(int)\n",
    "title_map = {'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4}\n",
    "X_kaggle[\"Title\"] = X_kaggle[\"Title\"].map(title_map).astype(int)\n",
    "\n",
    "# Align test features to match training set\n",
    "X_kaggle = X_kaggle.reindex(columns=X_all.columns, fill_value=0)\n",
    "\n",
    "# Retrain final model on the full training set\n",
    "final_model = GradientBoostingClassifier(random_state=42)\n",
    "final_model.fit(X_all, y_all)\n",
    "\n",
    "# Generate predictions on Kaggle test data\n",
    "kaggle_preds = final_model.predict(X_kaggle)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": df_test[\"PassengerId\"],\n",
    "    \"Survived\": kaggle_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255f15f",
   "metadata": {},
   "source": [
    "The submission was uploaded to Kaggle and received a public score of **0.79186**.  \n",
    "This score reflects the performance of the final GradientBoostingClassifier model trained on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c99898",
   "metadata": {},
   "source": [
    "### üß† Summary\n",
    "In this notebook, I finalized the Titanic survival prediction project by:\n",
    "\n",
    "- Selecting `GradientBoostingClassifier` as the final model based on F1 score and ROC AUC  \n",
    "- Retraining the model on the training set  \n",
    "- Generating predictions and saving the result as `submission.csv` in Kaggle format\n",
    "\n",
    "This marks the completion of the core workflow.  \n",
    "Future improvements could include hyperparameter tuning, model stacking, or feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc0f2d",
   "metadata": {},
   "source": [
    "## üì¶ Overall Summary\n",
    "\n",
    "This notebook concludes a 7-part beginner-friendly Titanic survival prediction project.\n",
    "\n",
    "Throughout the series, I explored:\n",
    "- Data cleaning and visualization (01‚Äì02)\n",
    "- Feature engineering (03‚Äì04)\n",
    "- Model building and evaluation (05‚Äì06)\n",
    "- Final model selection and submission preparation (07)\n",
    "\n",
    "By completing this project, I practiced core data science skills such as:\n",
    "- Exploratory data analysis (EDA)\n",
    "- Feature preprocessing and transformation\n",
    "- Model selection and evaluation using multiple metrics\n",
    "- Real-world prediction workflow aligned with Kaggle submission format\n",
    "\n",
    "This was my first fully documented project on GitHub,  \n",
    "and it helped me build both skills and confidence in the data science workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

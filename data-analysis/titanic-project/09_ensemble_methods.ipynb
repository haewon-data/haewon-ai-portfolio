{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a905e5",
   "metadata": {},
   "source": [
    "# ü§ù Ensemble Learning\n",
    "\n",
    "This notebook explores ensemble methods to boost performance  \n",
    "by combining multiple machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "To evaluate ensemble strategies‚ÄîVoting, Bagging, Boosting, and Stacking‚Äî  \n",
    "and compare them with the best single model from earlier work.\n",
    "\n",
    "Each method will be briefly introduced and tested using the same dataset and evaluation metrics.\n",
    "\n",
    "## üì¶ Dataset\n",
    "\n",
    "Same processed dataset used in earlier notebooks:  \n",
    "[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)  \n",
    "via public repository: [Data Science Dojo GitHub](https://github.com/datasciencedojo/datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7c576",
   "metadata": {},
   "source": [
    "üì¶ 1. Load the Feature-Engineered Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1ac79",
   "metadata": {},
   "source": [
    "I begin by loading the fully processed Titanic dataset,  \n",
    "which includes all engineered features created in earlier notebooks.\n",
    "\n",
    "I also prepare the input matrix `X` and target vector `y`  \n",
    "to be used consistently across all ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb2de849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed dataset with all engineered features\n",
    "df = pd.read_csv(\"feature_engineered_titanic.csv\")\n",
    "\n",
    "# One-hot encode 'Sex'\n",
    "df['Sex_male'] = (df['Sex'] == 'male').astype(int)\n",
    "df['Sex_female'] = (df['Sex'] == 'female').astype(int)\n",
    "df = df.drop(columns=['Sex'])\n",
    "\n",
    "# Define features and target\n",
    "expected_columns = [\n",
    "    'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "    'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E',\n",
    "    'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_U',\n",
    "    'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
    "    'FamilySize', 'IsAlone', 'LowFare', 'AgeBin',\n",
    "    'ModerateFamily', 'IsCherbourg', 'FemaleFirstSecondClass',\n",
    "    'IsChildOrElderly', 'LowFare_3rdClass',\n",
    "    'Sex_male', 'Sex_female', 'Age', 'Parch', 'SibSp'\n",
    "]\n",
    "X_all = df[expected_columns]\n",
    "y_all = df['Survived']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de305e",
   "metadata": {},
   "source": [
    "Instead of dropping unnecessary columns, I explicitly selected the features used during training<br> to avoid any mismatch in the number or order of features.<br>This ensures consistency between the training and prediction data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea37f1",
   "metadata": {},
   "source": [
    "üß™ 2. Ensemble Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc500c",
   "metadata": {},
   "source": [
    "I test several ensemble methods to boost predictive performance.  \n",
    "The order reflects increasing model complexity and learning power:\n",
    "\n",
    "| Step | Method            | Description                                     |\n",
    "|------|-------------------|-------------------------------------------------|\n",
    "| 2-1  | Voting Classifier | Simple average of multiple model predictions    |\n",
    "| 2-2  | Bagging           | Random Forest to reduce variance                |\n",
    "| 2-3  | Boosting          | Gradient-based models for higher accuracy       |\n",
    "| 2-4  | Stacking          | Combine base model predictions through a meta-learner for improved performance            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674aafcd",
   "metadata": {},
   "source": [
    "2-1. Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12d834",
   "metadata": {},
   "source": [
    "Voting is a simple ensemble technique that combines predictions from multiple models.  \n",
    "In **soft voting**, I average the predicted class probabilities and choose the class with the highest average.\n",
    "\n",
    "I selected three strong models that not only perform well individually,  \n",
    "but also differ in learning strategies ‚Äî allowing them to complement each other.\n",
    "\n",
    "This helps balance out the weaknesses of individual models and often leads to more stable performance.\n",
    "\n",
    "**Models Used:**\n",
    "- Random Forest (Bagging-based)\n",
    "- Gradient Boosting (Sequential boosting)\n",
    "- XGBoost (Optimized boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ee48846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Mean F1 Score = 0.7682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define individual models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Voting ensemble\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('rf', rf), ('gb', gb), ('xgb', xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "scores = cross_val_score(voting, X_all, y_all, cv=5, scoring=make_scorer(f1_score))\n",
    "print(f\"Voting Classifier: Mean F1 Score = {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83affd",
   "metadata": {},
   "source": [
    "2-2. Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f01f4",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique that trains multiple models on different random subsets of the data.  \n",
    "Each model learns independently, and their predictions are aggregated (e.g., by voting or averaging).\n",
    "\n",
    "**Random Forest** is a classic bagging model that builds multiple decision trees,  \n",
    "each trained on a different bootstrap sample and a random subset of features.\n",
    "\n",
    "This strategy helps:\n",
    "- Reduce **variance** by averaging diverse predictions\n",
    "- Prevent overfitting compared to a single decision tree\n",
    "- Maintain strong performance with relatively low tuning effort\n",
    "\n",
    "**Key Concept:**  \n",
    "By combining many \"weakly correlated\" trees, Random Forest achieves robust and stable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff2b4b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Bagging): Mean F1 Score = 0.7346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define Random Forest model (Bagging)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Evaluate with 5-fold cross-validation using F1 score\n",
    "rf_scores = cross_val_score(rf_model, X_all, y_all, cv=5, scoring=make_scorer(f1_score))\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest (Bagging): Mean F1 Score = {rf_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fc898",
   "metadata": {},
   "source": [
    "2-3. Boosting: Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d1138",
   "metadata": {},
   "source": [
    "Boosting is a powerful ensemble technique that builds models sequentially.  \n",
    "Each new model focuses on the mistakes made by the previous ones, reducing bias and improving overall accuracy.\n",
    "\n",
    "I test two popular boosting algorithms:\n",
    "\n",
    "**Models Used:**\n",
    "- GradientBoostingClassifier: Standard boosting method with decision trees  \n",
    "- XGBClassifier: An optimized version of gradient boosting with better performance and regularization\n",
    "\n",
    "These models often achieve top performance on structured datasets like Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d117706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: Mean F1 Score = 0.7472\n",
      "XGBoost: Mean F1 Score = 0.7471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define models\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Evaluate with 5-fold cross-validation\n",
    "gb_scores = cross_val_score(gb_model, X_all, y_all, cv=5, scoring=make_scorer(f1_score))\n",
    "xgb_scores = cross_val_score(xgb_model, X_all, y_all, cv=5, scoring=make_scorer(f1_score))\n",
    "\n",
    "# Print results\n",
    "print(f\"Gradient Boosting: Mean F1 Score = {gb_scores.mean():.4f}\")\n",
    "print(f\"XGBoost: Mean F1 Score = {xgb_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfacbe5",
   "metadata": {},
   "source": [
    "2-4. Stacking Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9b77f",
   "metadata": {},
   "source": [
    "Stacking is a more advanced ensemble method that combines multiple base models using a meta-model.\n",
    "Instead of voting or averaging, the meta-model learns how to best combine the predictions of the base models.\n",
    "\n",
    "This method often achieves higher accuracy by leveraging the strengths of different algorithms.\n",
    "\n",
    "Base Models:\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "\n",
    "Meta-model:<br>\n",
    "- Logistic Regression (simple and effective for final decision making)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa0b53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble: Mean F1 Score = 0.7693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking ensemble\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# Evaluate with 5-fold CV using F1 score\n",
    "stacking_scores = cross_val_score(\n",
    "    stacking, X_all, y_all, cv=5, scoring=make_scorer(f1_score)\n",
    ")\n",
    "\n",
    "print(f\"Stacking Ensemble: Mean F1 Score = {stacking_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acd747",
   "metadata": {},
   "source": [
    "üìä 3. Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c0a2f",
   "metadata": {},
   "source": [
    "After evaluating all ensemble methods using 5-fold cross-validation with the F1 score,  <br>\n",
    "I summarize the results below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2fa2ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Voting Ensemble</td>\n",
       "      <td>0.768189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging (Random Forest)</td>\n",
       "      <td>0.734582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boosting (Gradient Boosting)</td>\n",
       "      <td>0.747230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stacking Ensemble</td>\n",
       "      <td>0.756582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Mean F1 Score\n",
       "0               Voting Ensemble       0.768189\n",
       "1       Bagging (Random Forest)       0.734582\n",
       "2  Boosting (Gradient Boosting)       0.747230\n",
       "3             Stacking Ensemble       0.756582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Define stacking model separately\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='logloss'))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(class_weight='balanced')\n",
    ")\n",
    "\n",
    "\n",
    "# Store mean F1 scores directly\n",
    "model_scores = {\n",
    "    \"Voting Ensemble\": cross_val_score(\n",
    "        VotingClassifier(estimators=[\n",
    "            ('rf', RandomForestClassifier(random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('xgb', XGBClassifier(random_state=42, eval_metric='logloss'))\n",
    "        ], voting='soft'),\n",
    "        X_all, y_all, cv=5, scoring=scorer\n",
    "    ).mean(),\n",
    "    \n",
    "    \"Bagging (Random Forest)\": cross_val_score(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        X_all, y_all, cv=5, scoring=scorer\n",
    "    ).mean(),\n",
    "    \n",
    "    \"Boosting (Gradient Boosting)\": cross_val_score(\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        X_all, y_all, cv=5, scoring=scorer\n",
    "    ).mean(),\n",
    "    \n",
    "    \"Stacking Ensemble\": cross_val_score(\n",
    "        stacking,\n",
    "        X_all, y_all, cv=5, scoring=scorer\n",
    "    ).mean()\n",
    "}\n",
    "\n",
    "# Display as DataFrame (with clean index)\n",
    "score_df = pd.DataFrame(\n",
    "    list(model_scores.items()), columns=[\"Model\", \"Mean F1 Score\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39d162",
   "metadata": {},
   "source": [
    "Stacking and Voting achieved the best performance,<br> confirming that combining diverse models leads to stronger predictions than using a single method alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584713f8",
   "metadata": {},
   "source": [
    "üöÄ Exporting the Final Model for Real-World Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2dc4dc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../titanic-project/deployment/model.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "stacking.fit(X_all, y_all)\n",
    "\n",
    "joblib.dump(stacking, '../titanic-project/deployment/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba9c19",
   "metadata": {},
   "source": [
    "## üß† Summary\n",
    "\n",
    "In this notebook, I tested four ensemble methods to improve prediction performance:\n",
    "\n",
    "- **Voting**: Combined three well-performing, diverse models  \n",
    "- **Bagging**: Applied Random Forest to reduce variance  \n",
    "- **Boosting**: Used Gradient Boosting for sequential error correction  \n",
    "- **Stacking**: Blended base models using a meta-model\n",
    "\n",
    "Among all methods, **Voting** gave the best F1 score (0.768), followed closely by Stacking.  \n",
    "This suggests that combining models with different learning styles can lead to more robust results.\n",
    "\n",
    "It also confirmed that ensemble learning is a useful next step after building strong individual models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
